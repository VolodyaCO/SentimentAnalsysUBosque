{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer  # módulo para tokenizar los tweets\n",
    "from nltk.corpus import stopwords  # lista de palabras basura\n",
    "import unicodedata  # módulo para tratar caracteres unicode\n",
    "import language_check  # módulo para corregir los tweets\n",
    "from preprocessor.api import clean  # módulo para limpiar tweets INSTALAR: pip install tweet-preprocessor\n",
    "import re # Para trabajar con texto\n",
    "tknzr = TweetTokenizer()  # selecciona el tokenizador para tweets\n",
    "tool = language_check.LanguageTool('es')  # trabajador de language_check para correjir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacer esto, importante para bajar los data sets de NLTK!\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    \"\"\"\n",
    "    Quita los acentos para que el tokenizador de tweets no parta las palabras con tildes en 2.\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def remove_stopwords(sentence, language):\n",
    "    \"\"\"\n",
    "    Dada una frase, devuelve la frase tokenizada y sin palabras basura\n",
    "    \"\"\"\n",
    "    return [ token for token in tknzr.tokenize(strip_accents(sentence)) if token.lower() not in stopwords.words(language)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trists', 'trabajando', 'analisis', 'sentimientos']\n"
     ]
    }
   ],
   "source": [
    "frase = \"estamos trists trabajando en análisis de sentimientos\"\n",
    "tokens = remove_stopwords(frase, \"spanish\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(word):\n",
    "    \"\"\"\n",
    "    Corrije las palabras si es necesario\n",
    "    \"\"\"\n",
    "    matches = tool.check(word)\n",
    "    return language_check.correct(word, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trisas\n",
      "Trabajando\n",
      "análisis\n",
      "Sentimientos\n"
     ]
    }
   ],
   "source": [
    "for palabra in tokens:\n",
    "    print(correct_word(palabra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_correct_tokenize(text):\n",
    "    \"\"\"\n",
    "    Hace todo el procedimiento de limpiar, correjir, tokenizar, y sacar la raíz de las palabras.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    clean_ = clean(text)\n",
    "    matches = tool.check(clean_)\n",
    "    corrected = language_check.correct(text, matches).lower()\n",
    "    no_stop_words = remove_stopwords(corrected, 'spanish')\n",
    "    tokens = [stemmer.stem(correct_word(word).lower()) for word in no_stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tris', 'trabaj', 'analisis', 'sentimient']\n"
     ]
    }
   ],
   "source": [
    "ejemplo = clean_correct_tokenize(frase)\n",
    "print(ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir un archivo con texto\n",
    "\n",
    "with open(\"comentarios_p.txt\", \"r\", encoding='latin-1') as f:\n",
    "    texto = f.readlines()\n",
    "    \n",
    "frases_preprocesadas = [] \n",
    "frases_desnudas = []\n",
    "\n",
    "for sentence in texto: \n",
    "    tokenizado = clean_correct_tokenize(sentence)\n",
    "    frases_preprocesadas.append(tokenizado)\n",
    "    frases_desnudas.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"comentarios_preprocesados.txt\", \"w\") as f:\n",
    "    for frase in frases_preprocesadas:\n",
    "        nueva_frase = \" \".join(frase)\n",
    "        f.write(nueva_frase+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist, tokenize\n",
    "with open(\"comentarios_preprocesados.txt\", \"r\") as f:\n",
    "    texto = f.read()\n",
    "    palabras_preprocesadas = tokenize.word_tokenize(texto)\n",
    "    fdist = FreqDist(palabras_preprocesadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('univers', 108),\n",
       " ('estudi', 85),\n",
       " ('instal', 84),\n",
       " ('opin', 83),\n",
       " ('relacion', 72),\n",
       " ('profesor', 68),\n",
       " ('buen', 66),\n",
       " ('bosqu', 54),\n",
       " ('algun', 32),\n",
       " ('entabl', 31),\n",
       " ('espaci', 26),\n",
       " ('realiz', 26),\n",
       " ('mejor', 25),\n",
       " ('quej', 22),\n",
       " ('falt', 21),\n",
       " ('segur', 21),\n",
       " ('academ', 21),\n",
       " ('parec', 20),\n",
       " ('consej', 18),\n",
       " ('mas', 18),\n",
       " ('brind', 16),\n",
       " ('maton', 15),\n",
       " ('calid', 15),\n",
       " ('consider', 15),\n",
       " ('vez', 14),\n",
       " ('mayor', 14),\n",
       " ('recib', 14),\n",
       " ('evalu', 14),\n",
       " ('institu', 14),\n",
       " ('exist', 14),\n",
       " ('respuest', 13),\n",
       " ('si', 13),\n",
       " ('satisfech', 13),\n",
       " ('hac', 12),\n",
       " ('alumn', 12),\n",
       " ('dieron', 12),\n",
       " ('inquietud', 12),\n",
       " ('pued', 11),\n",
       " ('activ', 10),\n",
       " ('tem', 10),\n",
       " ('gust', 10),\n",
       " ('comput', 10),\n",
       " ('trabaj', 9),\n",
       " ('general', 9),\n",
       " ('estan', 9),\n",
       " ('agrad', 9),\n",
       " ('clas', 9),\n",
       " ('dud', 8),\n",
       " ('tal', 8),\n",
       " ('aunqu', 8)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras más comunes:\n",
    "\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semillas:\n",
    "\n",
    "positivas = [ 'buen', 'satisfech', 'mejor', 'gust', 'agrad']\n",
    "negativas = ['quej', 'falt', 'maton' , 'dud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(frases_preprocesadas, size=15, window=2, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'no' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5bb412b7b5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'no' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_positivas = []\n",
    "new_negativas = []\n",
    "for pos in positivas:\n",
    "    similar = model.wv.most_similar(positive=[pos], topn=3)\n",
    "    for palabra in similar:\n",
    "        palabra = palabra[0]\n",
    "        if len(palabra)>1:\n",
    "            new_positivas.append(palabra)\n",
    "            \n",
    "for neg in negativas:\n",
    "    similar = model.wv.most_similar(positive=[neg], topn=3)\n",
    "    for palabra in similar:\n",
    "        palabra = palabra[0]\n",
    "        if len(palabra)>1:\n",
    "            new_negativas.append(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivas += new_positivas\n",
    "negativas += new_negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivas = list(set(positivas))\n",
    "negativas = list(set(negativas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['escuch',\n",
       " 'much',\n",
       " 'mes',\n",
       " 'maton',\n",
       " 'instal',\n",
       " 'sin',\n",
       " 'ejempl',\n",
       " 'obvi',\n",
       " 'poc',\n",
       " 'quej',\n",
       " 'bosqu',\n",
       " 'van',\n",
       " 'dud',\n",
       " 'falt',\n",
       " 'institu']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "polaridades = []\n",
    "\n",
    "for frase in frases_preprocesadas:\n",
    "    polaridad = 0\n",
    "    for palabra in frase:\n",
    "        try:\n",
    "            model.wv[palabra]\n",
    "            if palabra in positivas:\n",
    "                polaridad += 1\n",
    "            elif palabra in negativas:\n",
    "                polaridad -= 1\n",
    "        except:\n",
    "            pass\n",
    "    polaridades.append(polaridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Cómo evaluaría la calidad de la consejería académica que recibió en la institución? Me gustan, me siento muy cómodo pero algunos baños del ¿Cómo evaluaría la calidad de la consejería académica que recibió en la institución? séptimo piso del E son insoportables por el olor a orines.\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_desnudas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polaridades[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
